{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d662429f-0f79-413e-9384-d6eca0a9ab85",
   "metadata": {},
   "source": [
    "# Exercise 2: Intermediate AD with Auto Encoders on Jets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77935500-dcf8-49a3-ad93-c4f119884f1f",
   "metadata": {},
   "source": [
    "### Goals of the exercise\n",
    "\n",
    "- Create a basic dense auto-encoder for jets\n",
    "- Create a whole event 1D \"Time Distributed\" Auto-encoder for event jet anomaly detection\n",
    "- Create a 1D knowledge distillation model with some DeepSets properties\n",
    "\n",
    "### Anomaly Detection With Autoencoders\n",
    "\n",
    "**I am going to write a whole lot here about autoencoders, anomaly detection with autoencoders, and a few asides on autoencoders here. If you are already well familiar with autoencoders and the concept of anomaly detection with them, you can skip ahead a few sections**\n",
    "\n",
    "Okay, as is pretty much an inescapable fact of life in this field, and every field right now, neural networks are dominating the landscape, they are in all the buzzwords, and everyone wants to talk about neural networks and AI. Well, neural networks are of course used for anomaly detection too. The primary method is with auto-encoders.\n",
    "\n",
    "I am sure that everyone has heard it by now, either before this workshop, or certainly _in_ this workshop, but an auto encoder is a neural network that is designed to reproduce it's own inputs. The thing that prevents this from being trivial, and prevents the neural network to collapsing to the identity function, is that we force it to go through a reduced information space (between the \"encoder\" and \"decoder\") called the \"latent space\". The idea behind this being that to be able to effectively learn to recreate it's inputs, it has to learn an efficient representation of the data it is looking at (the \"encoding\") or put another way, it has to learn something about the structure of the data it is looking at. The extension to anomaly detection comes when we start applying this auto-encoder to other data. If it is not from our dataset, we would expect (or hope) that this representation of the data fails, and the reconstructed data is noticeably worse. The quality of the reconstruction then becomes a metric for the anomalous-ness, the \"anomaly score\".\n",
    "\n",
    "##### Aside: Out Of Set Reconstruction\n",
    "This doesn't always work. Sometimes, to everyone's chagrin, you will find that the auto-encoder can work _better_ on data it has not seen. It's one of those annoying features of the expressive power of neural networks, that the patterns you hope it picks up on, are not always the pattern it does pick up on. For example, consider an auto-encoder trained to reconstruct 50 random numbers from a latent space of 20. There's not much strategy or structure to this, the single best thing you can do to be the most right is just memorize the 20 largest numbers in the set and pass those on, and guess the average for the rest. There is something to be learned while the neural network picks up on this strategy and forms the functions that approximate it in the background, but later, when given a set of 50 random numbers with higher peaks, but a lower average, you'll find it doesn't register these random spikes as anomalies because it memorizes them quite well, and it thinks the data is less anomalous than its training set!\n",
    "\n",
    "This general problem is called \"Out Of Set Reconstruction\" and really deserves much more time than I can give it here. There are a number of strategies for handling it, you can change your network architecture, restrict the latent space harshly, change loss functions, try to introduce noise and drop-out, (ask ChatGPT \"how do I avoid out of set reconstruction on this dataset\" _but do the research on what it suggests to make sure it isn't just making things up_...) or change your strategy for what it is you are encoding. It's its own art-form. For now you should just be aware the issue exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d5b54-ec8f-468d-9c52-8500881d5cf8",
   "metadata": {},
   "source": [
    "### Where Was I? Right, Autoencoders\n",
    "\n",
    "Basic auto-encoders are flat, and use dense layers like basic regression or classification neural networks. Typically they will include a few dense layers that get narrower into a latent space, then a mirror going back to the original size and shape, the first half being the \"encoder\" the second half the \"decoder\". There are a great number of variations on this concept. Some designs introduce noise and dropout into the encoder, forcing the autoencoder to learn only the most pertinent things, and get rid of noise (called a \"denoising auto-encoder\"). Some designs force extremely sparse latent spaces, so that only a few features at a time are used in the latent space to try and force pertinent feature construction, (to the point sometimes in these types of autoencoders that the latent space will be _larger_ than the input space, because it is so much less populated, these are called \"sparse autoencoders\"). A particularly useful kind makes it's latent space probabilistic, having normally distributed latent space features, being encoded as a mean and standard deviation, the resulting distribution being sampled from to decode it (the \"variational autoencoders\")\n",
    "\n",
    "##### Aside: Variational Autoencoders as Generators\n",
    "\n",
    "if you are unfamiliar with auto-encoders, especially variational ones, early generative AI stuff made heavy use of variational autoencoders. It might still, I am not super up to date on whatever models they use to make generated images. The idea was, you fed it tons upon tons of images, through a variational autoencoder, then you took only the decoding half of the model. You could feed random noise as the parameters of the distribution to get random images that looked like the images it was learning to encode, or if you studied a bit what kinds of images corresponded to what encoded bits of latent space, you could shape the noise you gave it to get images that looked like certain images you wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab7020-44e1-420d-b975-ca00e6c99459",
   "metadata": {},
   "source": [
    "### Let's Do Some Machine Learning with Autoencoders\n",
    "\n",
    "For this exercise, I have a similar set of datasets to before, here: `/data/intermediate_files/`. These files do not have multiplicities, but instead a bunch of jet data in them, sorted by event.\n",
    "\n",
    "The exact files are:\n",
    "- `data/intermediate_files/ZeroBiasIntermediateData.h5`\n",
    "- `data/intermediate_files/JetHTIntermediateData.h5`\n",
    "- `data/intermediate_files/TTBarIntermediateData.h5`\n",
    "- `data/intermediate_files/SoftQCDIntermediateData.h5`\n",
    "- `data/intermediate_files/RadionIntermediateData.h5`\n",
    "\n",
    "The physics of these files and why they were selected are in the Basic AD notebook.\n",
    "\n",
    "The contents of the files have changed though. Now per entry there will be 8 vectors, each a jet (sorted in leading $p_{T}$ order, of 17 features each (0's if the jet does not exist).\n",
    "\n",
    "These 17 features are (in index order):\n",
    "\n",
    "0. Jet $p_{T}$\n",
    "1. Jet $\\eta$\n",
    "2. Jet $\\phi$\n",
    "3. Jet Mass\n",
    "4. Number of Jet Constituents\n",
    "5. Number of electrons in the jet\n",
    "6. Number of muons in the jet\n",
    "7. Jet charged hadron fraction\n",
    "8. Jet neutral hadron fraction\n",
    "9. Jet DeepCSV b+bb score (a CMS neural network classifier for b-tagging jets)\n",
    "10. Jet DeepCSV c vs b score (a CMS neural network classifier for tagging c jets vs b jets)\n",
    "11. Jet DeepCSV c vs udsg score (a CMS neural network classifier for tagging c jets vs light jets)\n",
    "12. Jet catchment area\n",
    "13. Jet DeepFlav b+bb score (a more modern neural network for b+bb tagging)\n",
    "14. Jet DeepFlav c vs b score\n",
    "15. Jet DeepFlav c vs light score\n",
    "16. Jet DeepFlav g vs uds score\n",
    "\n",
    "These variables were selected at random but as a pretty representative set of features that encompass jet information at CMS.\n",
    "\n",
    "Let's load this info, and get some necessary packages\n",
    "\n",
    "(because there is so much data here, I had to limit how much of it we load and use, these will be a bit smaller than before)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11e10d-bec4-42d2-9585-b92a1a39dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9331f9-3e2f-45bd-b725-5bdc30b4a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, limit_entries: int = None) -> np.array:\n",
    "    with h5py.File(file_path) as the_file:\n",
    "        if limit_entries is not None:\n",
    "            data = np.array(the_file['jet_event_data'])[:limit_entries]\n",
    "        else:\n",
    "            data = np.array(the_file['jet_event_data'])\n",
    "    return data\n",
    "\n",
    "zerobias_jets = load_data('data/intermediate_files/ZeroBiasIntermediateData.h5')\n",
    "\n",
    "console.print(zerobias_jets.shape)\n",
    "\n",
    "jetht_jets = load_data('data/intermediate_files/JetHTIntermediateData.h5')\n",
    "\n",
    "console.print(jetht_jets.shape)\n",
    "\n",
    "ttbar_jets = load_data('data/intermediate_files/TTBarIntermediateData.h5')\n",
    "\n",
    "console.print(ttbar_jets.shape)\n",
    "\n",
    "softqcd_jets = load_data('data/intermediate_files/SoftQCDIntermediateData.h5')\n",
    "\n",
    "console.print(softqcd_jets.shape)\n",
    "\n",
    "radion_jets = load_data('data/intermediate_files/RadionIntermediateData.h5')\n",
    "\n",
    "console.print(radion_jets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a005bb-f810-42c6-9433-83aaad62e259",
   "metadata": {},
   "source": [
    "These jets are laid out in a sequence in each event. This will be useful in a bit, but for now, we don't need the jets in any sequence, we just need individual jets, so we can flatten these out. Let's also get rid of any jet with $p_{T} = 0$, since these were empty jet slots that didn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c1f22-8ef5-481e-b8ad-d97fdfdd5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_jets(jets:np.array)->np.array:\n",
    "    return jets.reshape((-1,17))\n",
    "\n",
    "def filter_zero_jets(jets:np.array)->np.array:\n",
    "    return jets[jets[:, 0] != 0.0]\n",
    "\n",
    "zerobias_flat_jets = filter_zero_jets(flatten_jets(zerobias_jets))\n",
    "jetht_flat_jets = filter_zero_jets(flatten_jets(jetht_jets))\n",
    "ttbar_flat_jets = filter_zero_jets(flatten_jets(ttbar_jets))\n",
    "softqcd_flat_jets = filter_zero_jets(flatten_jets(softqcd_jets))\n",
    "radion_flat_jets = filter_zero_jets(flatten_jets(radion_jets))\n",
    "\n",
    "console.print(zerobias_flat_jets.shape)\n",
    "console.print(jetht_flat_jets.shape)\n",
    "console.print(ttbar_flat_jets.shape)\n",
    "console.print(softqcd_flat_jets.shape)\n",
    "console.print(radion_flat_jets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3a96c-bdd6-47c5-9cc4-0c20a060ddee",
   "metadata": {},
   "source": [
    "I am also going to split the zero bias jets up into a training, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d96cba-8519-442c-b91e-175d2e7ca212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zerobias_train, zerobias_testval = train_test_split(zerobias_flat_jets,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state = 42)\n",
    "zerobias_val, zerobias_test = train_test_split(zerobias_testval,\n",
    "                                               test_size=0.2/0.4,\n",
    "                                               random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98509d-dbc2-4210-8f3c-0d870c4e6fa2",
   "metadata": {},
   "source": [
    "Now, let's build an auto-encoder! This is where you step in.\n",
    "\n",
    "This is simple enough to do with Tensorflow/Keras, which is my preferred library over pytorch for simple neural network applications (pytorch is a bit more flexible admittedly).\n",
    "\n",
    "Make a neural network auto encoder model. Don't worry about compiling it or losses for right now. We'll talk about those in a second.\n",
    "\n",
    "if you haven't built a neural network with keras before, let me know and we can go over the basics together. If you just need a refresher on what the API is like and what layers are what and go where, [this is the keras layers API](https://keras.io/api/layers/), [this is the model API](https://keras.io/api/models/) and [here is the callbacks API](https://keras.io/api/callbacks/) for later. A simple `Sequential` model will suffice here.\n",
    "\n",
    "Some things to think about:\n",
    "\n",
    "How big should the layers be? The only real restriction is that the latent space has to be smaller than the input one... but that's not much of a restriction. The temptation is to leave them pretty large, after all most reconstruction losses will be better that way, but we're not necessarily after the best reconstruction loss here, but instead it's ability to capture the most salient features and find outliers which is not necessarily the same thing. On the other hand, too small and you have no ability to capture much useful information. And how about intermediate encoding and decoding layers? If this were a real big project with time and resources, my recommendation would be to do some hyperparameter tuning ([keras-tuner](https://keras.io/keras_tuner/) is great for it) around some classifier like metrics on representative signal samples (that you don't train on to be clear!). Things like tuning the encoder for ROC AUC, Binary Cross Entropy, or Hinge Loss. For right now, just some guesses and starting points are good. I find a good starting place is to step down by powers of 2. In this case I would do something like a layer at 16, a final encoding layer about 8 units big, then the reverse, a layer at 16, then one at 17 units big.\n",
    "\n",
    "Seasoned veterans of neural networks may also ask \"what about regularization?\". It's a good question. Neural networks are very prone to overfitting. My advice is go for it, but see how it works both ways (neural networks are like that I guess). Dropout early in the encoder is denoising technique. You could also add Gaussian Noise (I have less luck with this, but the standard wisdom says it's useful for denoising auto-encoders too). L2 regularization is also a powerful option even in something as complicated as a neural network.\n",
    "\n",
    "The data I have given you is not normalized. You could normalize it yourself either by hand, or with a convenient sci-kit learn tool... but there are layers that will do that (or approximately that) for you. I would add `BatchNormalization`. Just remember it's there when you use the model later! _**However, put a big old pin in this topic, because I have a big aside from experience on the topic of normalizing data for anomaly detection purposes**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb026c5e-726b-4b52-803d-89a4022d4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a flat/dense autoencoder for jet information\n",
    "#\n",
    "flat_ae_model = keras.Sequential([\n",
    "    #\n",
    "    #encoder\n",
    "    #\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    # fill in remaining encoding steps here (hint: dense layers with normalization layers in between!)\n",
    "    #\n",
    "    #decoder\n",
    "    #\n",
    "    # fill in decoding steps here\n",
    "    keras.layers.Dense(17), #no activation function to allow for [-inf, inf]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f6b80-e87b-428b-946f-56519b359dcf",
   "metadata": {},
   "source": [
    "Okay. Now that we have a model, let's talk about a loss function, since the whole technique of anomaly detection hinges on using the loss function. Since this is technically a regression/reconstruction task, Mean Squared Error (MSE) is a standard, and I would recommend it to start here too. You could also use Mean Absolute Error. A rarer choise would Mean Absolute Percentage Error (providing you can use it and no output feature will be 0 or close to 0), but as part of the aside I mentioned above, I'll at least mention it as an option.\n",
    "\n",
    "##### Aside: Anomaly Detection and Event Normalization\n",
    "\n",
    "So while I was talking about normalization above, I mentioned `BatchNormalization`. This normalizes an event _per feature_. That is to say, that feature 1 gets normalized with respect to feature 1, so on and so forth. The slight catch to this is it means that the normalization of the entire entry technically has a floating normalization. What do I mean by that? Imagine our jet had 3 features, $p_{x}$, $p_{y}$ & $p_{z}$, if we do batch normalization, nothing prevents an individual jet with large momentum from having all 3 of those features be large individually. The features are normalized with respect to themselves, not the entry. \n",
    "\n",
    "Now the catch, and my experience, is that because the entry has \"more in it\" so to speak, there is more for the auto-encoder to get wrong. It isn't going to learn anything exactly so a jet with \"more in it\" is typically going to have a higher loss because there is more to get wrong and is going to show up as more anomalous (globally louder entries $\\rightarrow$ more anomalous to put it simply). Similarly, you have to be careful with _feature_ normalization because features with inherently higher normalization may end up contributing more to the overall anomaly score than something else.\n",
    "\n",
    "This _may_ not be an issue. In this example, it is not unreasonable that we say \"yeah, high $p_{T}$ jets should be more anomalous\", but you should be aware of this effect. If you don't want to consider the overall normalization of an object/event/whatever in your anomaly detection, you need to find ways to mitigate this. One is using `LayerNormalization` cleverly, normalizing every object/event/whatever such that the event always has an overall normalization of 0 or close to 0. Another is using scale invariant losses like Mean Absolure Percentage Error. To avoid the feature issue, you can normalize features before considering anything, and only do anomaly scores and losses with respect to the already normalized features.\n",
    "\n",
    "Just think about what kinds of events and objects may be contributing to your loss, and what your loss means/rewards/rates as anomalous, especially if you are trying to interpret things as _structurally anomalous_ versus _magnitude or overall anomalous_ (\"anomalous\" is one of those things that permits some interpretation)\n",
    "\n",
    "### Back to the show\n",
    "\n",
    "Let's compile this, and use Mean Absolute Error as a metric (use Adam or Nadam as an optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce0734-cbc1-485b-a0ae-a9e1849f44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: compile the neural network model\n",
    "#\n",
    "flat_ae_model.compile(\n",
    "    # fill in here! specify loss function, optimizer, metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29f72d-2316-4f02-92a4-227de2d7e884",
   "metadata": {},
   "source": [
    "Now, let's get to training! I love this part.\n",
    "\n",
    "You can train for as long as you like (a neural network expert I used to know always told me to just train models for as long as you have and as much CPU power as you have until you get bored or need to use it, so long as you save the best model). A simple choice would be to just do 10-20 epochs or something. Myself, I would probably set it to something large like 200 epochs, but use a callback (`EarlyStopping` with it's `restore_best_weights` option to just train until it plateaus or starts overfitting, and then automatically get the best one back without having to save anything to disk right now)\n",
    "\n",
    "If you're new to auto-encoders, the `X` and `Y` in the `fit` call are the same thing here! Remember to use the validation data too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956367c-8721-447b-89e5-2ff1bef081e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: fit your flat jet autoencoder (fill in arguments)\n",
    "#\n",
    "flat_ae_model.fit(\n",
    "    x=,\n",
    "    y=,\n",
    "    epochs=,\n",
    "    validation_data=,\n",
    "    callbacks= # if using large number of epochs as explained above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98359743-0047-47da-ad09-9eb1dfbeb786",
   "metadata": {},
   "source": [
    "Alright. Let's see how we did.\n",
    "\n",
    "First thing's first, you should evaluate this on the zerobias test set to make sure the performace is what we think it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443aa9c-19d2-4f6d-86c1-9b58ad0c1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Evaluate the model using `evaluate` (fill in arguments)\n",
    "#\n",
    "\n",
    "flat_ae_model.evaluate(\n",
    "    x=, \n",
    "    y=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba75e3-f1c6-45d3-9103-cb291a14bfdc",
   "metadata": {},
   "source": [
    "If you've set up a model like me, I get an MSE loss of about $\\approx 0.07$. That may depend on the specifics of your network though.\n",
    "\n",
    "There's a lot of things I am not going to have time to get into. A fun thing to do would be to go through and look at jet reconstructions and see what things this auto-encoder gets wrong and where, and see if it gets things wrong in tandem with each other and so on, so in classic physics style I guess I'll leave that problem for the student. Let's just quickly take a look at how it performs on one jet though\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14714e66-c6b1-403b-b640-0225ac8aaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(flat_ae_model.predict(zerobias_val[0:1]))\n",
    "console.print(zerobias_val[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5839ac-d670-4b50-b26b-b8418cb8adb7",
   "metadata": {},
   "source": [
    "What we should do is start using this for anomaly detection. Which means that we need anomaly scores, which are our losses. I'll give you this utility for making anomaly scores, asuming you are using Mean Squared Error as your loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18eea2-2d5e-4e06-a73e-d6cd88b60f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anomaly_scores(model, input_dataset: np.array) -> np.array:\n",
    "    predictions = np.array(model.predict(input_dataset))\n",
    "    mse = np.mean((predictions-input_dataset)**2, axis=1)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba270dca-74a1-4c92-aab8-7ef0463f9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: get anomaly scores for all distributions (fill in the blanks)\n",
    "#\n",
    "\n",
    "zerobias_anomaly_scores = make_anomaly_scores(flat_ae_model, zerobias_test)\n",
    "jetht_anomaly_scores = \n",
    "ttbar_anomaly_scores =\n",
    "softqcd_anomaly_scores = \n",
    "radion_anomaly_scores = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9680a6-0931-41c0-baad-abc5270ec54d",
   "metadata": {},
   "source": [
    "As before, a really good thing to do is check out anomaly scores or anomalousness against certain features. Get some scatter plots on the zero bias test set between anomaly score, and jet $p_{T}$, $\\eta$, number of constituents, and the DeepFlav b tag score. Also, calculate the (Pearson) correlation coefficients. Let's see what \"anomalous\" is to this autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2f849-8beb-4514-be2d-460b08e49644",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(np.mean(zerobias_anomaly_scores))\n",
    "console.print(np.std(zerobias_anomaly_scores))\n",
    "console.print(np.mean(jetht_anomaly_scores))\n",
    "console.print(np.std(jetht_anomaly_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83173e-b3e2-42cc-b3ae-77f2c1b95ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Create pearson correlation coefficients of anomaly scores\n",
    "#\n",
    "\n",
    "for feature in range(17):\n",
    "    correlation_coeff = # use np.corrcoef function to get correlation coeffs for each feature\n",
    "    console.print(f'Anomaly score correlation to feature {feature}: {correlation_coeff:.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5ac3f-69dc-40e3-88b8-a6aa8cc6f408",
   "metadata": {},
   "source": [
    "I find in my model that the anomaly score is most correlated to the jet's $p_{T}$, the jet's mass, and the jet's b-tagging score (followed by charm versus light tagging and number of constituents). Not unreasonable (but note that both $p_{T}$ and $m$ are two of the higher overall variance features...). However, we should take a direct look at some of them, because correlation coefficient doesn't catch non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadedd77-d95e-49d9-9b12-569c22923043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Look at zerobias anomaly scores versus certain features in scatter plots\n",
    "#\n",
    "\n",
    "plt.scatter(\n",
    "    # pT vs anomaly score\n",
    ")\n",
    "\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel(r'$p_{T}$')\n",
    "plt.title('anomalousness versus Jet PT')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(\n",
    "    # number of constituents vs anomaly score\n",
    ")\n",
    "\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel(r'nConstituents')\n",
    "plt.title('anomalousness versus Jet nConstituents')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(\n",
    "    # b tag vs anomaly score\n",
    ")\n",
    "\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel(r'b tag score')\n",
    "plt.title('anomalousness versus DeepFlav b tag score')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# try looking at other features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149f674-7a3f-4050-8c4e-142004006a46",
   "metadata": {},
   "source": [
    "With my model, I find that the anomaly score is higher for jets with **too low or too high nConstituents**\n",
    "\n",
    "Alright, like we did in the basic exercises, let's check whether jets not from Zero Bias (i.e. straight from the CMS beam with no preselection) look more anomalous to us. Let's make score plots per sample, and then make a ROC curve comparing the JetHT dataset, $t\\bar{t}$ and radion to Zero bias, a ROC curve comparing $t\\bar{t}$ and radion to JetHT, and a ROC curve comparing $t\\bar{t}$ and radion to soft qcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500fe5a-8569-4864-a783-bc72fa9f47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make score plots for each sample\n",
    "#\n",
    "\n",
    "plt.hist(\n",
    "    zerobias_anomaly_scores,\n",
    "    bins=20,\n",
    "    range=(0.0, 500.0),\n",
    "    alpha=0.5,\n",
    "    histtype='stepfilled',\n",
    "    density=True,\n",
    "    label='Zero Bias Dataset'\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # jet HT\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # ttbar\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # soft QCD\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # radion\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Anomaly Scores of jets in the dataset\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.title(\"Anomalous Jet score distribution\")\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52096f-2568-41f7-8b92-5585aba598a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Create ROC plots for the jet distributions for different datasets versus zero bias\n",
    "# hint: use what we learned in the basic tutorial to complete the make_roc method!\n",
    "#\n",
    "\n",
    "def make_roc(background_scores, signal_scores, legend_label) -> None:\n",
    "    y_true = # fill in\n",
    "\n",
    "    y_pred = # fill in\n",
    "\n",
    "    fpr, tpr, _ = # fill in\n",
    "    auc = # fill in\n",
    "\n",
    "    plt.plot(\n",
    "        # fill in\n",
    "    )\n",
    "\n",
    "make_roc(zerobias_anomaly_scores, jetht_anomaly_scores, 'Jet HT Dataset')\n",
    "make_roc() # fill in for ttbar\n",
    "make_roc() # fill in for soft qcd\n",
    "make_roc() # fill in for radion\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1579f0e-d542-4f01-a8b6-ba32b3910135",
   "metadata": {},
   "source": [
    "We find the interesting datasets' jets do have some good anomalousness to this model, but not _terrific_ (again, we do well on a wide set of things, but are not super performers on any one thing, that's the nature of these minimally biased techniques). In my ROCs I found that it was basically impossible to tell the difference between soft QCD and zero bias jets, and the best performance/most anomalous jets were for $t\\bar{t}$. \n",
    "\n",
    "There's more we could do here, but we need to move on.\n",
    "\n",
    "One of the criticisms of this model, is that it is pretty isolated. It considers one jet a time. The anomalous-ness is per jet not per event and that may help us in finding interesting physics. This is why these jets are stored in sets. We are going to make a 1D auto-encoder.\n",
    "\n",
    "Now most 1D auto-encoders are 1D convolutional auto-encoders, and most 1D convolutional models and indeed most 1D data in machine-learning assumes the single dimension is time. You will often hear it referred to as the time axis or the time dimension. It doesn't need to be, and it won't be in our case, but I wanted to bring it up to be clear.\n",
    "\n",
    "But because we're not doing anything in time, Convolutional auto-encoders don't make sense in our case. The relative adjacency of the jets in the set really doesn't have much to do with each other (sure it has a bit, being $p_{T}$ ordered, but not so much as to really justify convolutional treatment, and not really recurrent treatment either). However, a common thing in 1D neural networks are what are called \"time distributed layers\". In effect, a time distributed layer treats each step as a single flat set of data, and we train the same flat layer for each \"time\" step. We can then train an entire auto-encoder on all these jets at once, and get an _overall_ view of how anomalous we think the whole event jet setup was!\n",
    "\n",
    "Setting that up is your next exercise. [This is the time distributed layer documentation](https://keras.io/api/layers/recurrent_layers/time_distributed). Split up the data, make the model, compile it, and train it.\n",
    "\n",
    "(note, because tensorflow/keras needs to know the input shape of every event, we can't get rid of the \"empty\" jets here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104970e-9fba-43d0-a62b-4975cafb1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Using time distributed dense layers, make a 1D auto encoder\n",
    "#\n",
    "\n",
    "zerobias_train, zerobias_testval = train_test_split(zerobias_jets,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state = 42)\n",
    "zerobias_val, zerobias_test = train_test_split(zerobias_testval,\n",
    "                                               test_size=0.2/0.4,\n",
    "                                               random_state = 123)\n",
    "\n",
    "console.print(zerobias_train.shape)\n",
    "\n",
    "ae_model = keras.Sequential([\n",
    "    #\n",
    "    # encoder\n",
    "    #\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.TimeDistributed(\n",
    "        # fill in!\n",
    "    ),\n",
    "    # complete the rest of the encoding steps here\n",
    "    #\n",
    "    # decoder\n",
    "    #\n",
    "    # complete the decoding steps here\n",
    "    keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(17), #no activation function to allow for [-inf, inf]\n",
    "    )\n",
    "])\n",
    "\n",
    "ae_model.compile(\n",
    "    # fill in arguments!\n",
    ")\n",
    "\n",
    "ae_model.fit( # fill in arguments!\n",
    "    x=,\n",
    "    y=,\n",
    "    epochs=,\n",
    "    #steps_per_epoch=,\n",
    "    validation_data=,\n",
    "    callbacks=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278c8b1-13bd-4420-982c-8f1a596770cc",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "\n",
    "It's great that we have that big model for doing event wide stuff, but calculating the loss each time is time consuming and clunky. What's more, the reconstructed jet or event we make doesn't serve much purpose, except to see how wrong we got it. A technique you can use, especially in resource constrained environments like an FPGA, is to build a model that \"skips straight to the answer\". That is, you train a model that predicts from the inputs, what the loss of the auto-encoder would be. This technique is called \"knowledge distillation\". The basic idea is that you train a big anomaly detection auto-encoder, then have it make a bunch of losses on a training set. From that set of losses, and the inputs that made them, you have a new model start from those inputs, and predict the loss.\n",
    "\n",
    "We're going to do that with our big, event based network. It would be good also for this network to be order invariant. The original auto-encoder and the data is sorted via $p_{T}$ but it would be good for the anomaly score to not depend on that. To achieve that, we're going to use more `TimeDistributed` layers, which are order invariant, and we're going to be clever about picking our 1D $\\rightarrow$ flat function/layer to be one that is order invariant (`GlobalMaxPooling1D` or `GlobalAveragePooling1D`). This is adjacent to a technique for processing set based data called DeepSets. \n",
    "\n",
    "Strictly, to avoid overfitting issues, we should have split the dataset 4 ways at the start, one auto-encoder training set, one knowledge distillation training set, a validation set, and a testing set. We're going to just re-use the training set, but don't do that for real things. Let's make a network and do a first training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67393f-4c19-490d-8d3d-f3d3fb4aff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: make the anomaly scores for the training, test and validation set, and the signal distributions too\n",
    "#\n",
    "\n",
    "def make_event_anomaly_scores(model, input_dataset):\n",
    "    predictions = # compute predictions for input_dataset using model, then reshape to match input_dataset\n",
    "    mse = # compute MSE using the difference between predictinos and input_dataset\n",
    "    return mse\n",
    "\n",
    "zerobias_score_train = make_event_anomaly_scores(ae_model, zerobias_train)\n",
    "zerobias_score_val = # fill in\n",
    "zerobias_score_test = # fill in\n",
    "\n",
    "jetht_score = make_event_anomaly_scores(ae_model, jetht_jets)\n",
    "ttbar_score = # fill in\n",
    "softqcd_score = # fill in\n",
    "radion_score = # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732ec98-d409-420c-a7b5-083684d6dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make a knowledge distillation model, using time distributed layers and GlobalMaxPooling1D, and train it\n",
    "#\n",
    "\n",
    "simple_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    # fill in other layers here!\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "    # fill in other layers here!\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "simple_model.compile(\n",
    "    # fill in\n",
    ")\n",
    "\n",
    "simple_model.fit( # fill in\n",
    "    x=,\n",
    "    y=,\n",
    "    epochs=,\n",
    "    #steps_per_epoch=,\n",
    "    validation_data=,\n",
    "    callbacks=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1042c-235d-4561-a85d-8c6041bcf18c",
   "metadata": {},
   "source": [
    "Let's be sure to evaluate it too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308cf92-4230-4218-9320-b7a2f674ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: evaluate the knowledge distillation model on the test set\n",
    "#\n",
    "simple_model.evaluate(\n",
    "    # fill in!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d333142-e3cf-454f-a3b2-8627e96269c3",
   "metadata": {},
   "source": [
    "Your model might see a suprisingly high error predicting the anomaly score here, compared to what the validation loss might make you think. Why is that?\n",
    "\n",
    "One of the things we should be sure of is that the model performance is relatively _even_. Because anomalies are rarer by definition, they are underrepresented in our training data, and thus the model is likelier to be less accurate for those... precisely where we want the _most_ accuracy. Make some plots showing the knowledge distillation model error versus true anomaly score on zero bias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ceef28-43fa-4a3d-9eb0-b2387305999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: make some scatter plots of the error in the knowledge distillation model prediction versus true anomaly score\n",
    "#\n",
    "\n",
    "zerobias_prediction = np.array(simple_model.predict(zerobias_test)).reshape(zerobias_score_test.shape)\n",
    "zerobias_error = (zerobias_prediction-zerobias_score_test)\n",
    "#ttbar_prediction = np.array(simple_model.predict(ttbar_jets)).reshape(ttbar_score.shape)\n",
    "#ttbar_error = (ttbar_prediction - ttbar_score)\n",
    "\n",
    "plt.scatter(\n",
    "    # zero bias scores vs zero bias error\n",
    ")\n",
    "\n",
    "#plt.scatter(\n",
    "#    # ttbar scores vs ttbar errors\n",
    "#)\n",
    "\n",
    "plt.ylabel('Knowledge distillation error')\n",
    "plt.xlabel('Anomaly score')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc9fd3-438f-4c6f-a22a-7de53d715ad8",
   "metadata": {},
   "source": [
    "There are a few ways around this. The first is to cleverly pick the events you use for training, to assure that there is an even representation of anomaly scores (and other secondary variables you may want even performance in). The upside is that you can get good consistent performance from this. The downside is that you are running the risk of introducing bias, or other bad training behaviors by deliberately affecting the training data this way. The other method is to provide sample weights for the training data to ensure that there is roughly even weighting for different anomaly scores. The upside to this is that you aren't potentially messing with introducing biases into the dataset, just re-using and reweighting the one you have. The downside is that this isn't solving the statistics problem, just sort of trying to cover it up, and by giving some events outsize importance, you are running into some inconsistency problems. \n",
    "\n",
    "We won't have time to fully get into both of these here, but be aware of those as options.\n",
    "\n",
    "Now, as we have done before, let's check out the model performance on samples of interest! Make some ROCs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab19af9-9a4b-40b7-b921-a97c382f9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: make some ROCs for the knowledge distillation model\n",
    "#\n",
    "ttbar_prediction = np.array(simple_model.predict(ttbar_jets)).reshape(ttbar_score.shape)\n",
    "jetht_prediction = # fill in\n",
    "softqcd_prediction = # fill in\n",
    "radion_prediction = # fill in\n",
    "\n",
    "make_roc(zerobias_prediction, jetht_prediction, 'Jet HT Dataset')\n",
    "# call make_roc for ttbar, softqcd, and radion as well\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93dd47-a9ba-4390-afc2-9593a2f923ca",
   "metadata": {},
   "source": [
    "In spite of the possibly poor accuracy to anomalies, you will likely find that it does a pretty good job in the ROC plots and in the ROC AUC's (potentially even outperforming the autoencoder it was based on! This can happen due to some simple patterns it picks up, like \"higher energy = more anomalous in general\" where the auto-encoder may learn to fit some of those too well, the knowledge distillation model is simpler). The knowledge distillation can be a bit disappointing in terms of overall accuracy to the autoencoder, but it can maintain the _order_ of scores (signals higher, backgrounds lower), which is what matters in ROCs (and setting thresholds for use and things like that)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d022fe-549c-47fc-bb9e-d589ce709937",
   "metadata": {},
   "source": [
    "Another \"exercise for the reader\" we could do here is retrain the model on the dedicated Jet HT set and see how that model compares.\n",
    "\n",
    "### Wrap-up\n",
    "\n",
    "These are some basic (and slightly more advanced) ideas in anomaly detection for objects and events. Let's get into some advanced ideas for anomaly detection with whole events, and other types of neural networks in the next exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
