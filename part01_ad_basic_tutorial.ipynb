{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c02f384-407e-4654-a558-602d1152f24f",
   "metadata": {},
   "source": [
    "# Exercise 1: Basic Anomaly Detection with Non-Neural Network Methods\n",
    "\n",
    "### Goals of the exercise\n",
    "\n",
    "- Use some basic python tools common to most machine learning and anomaly detection\n",
    "- Use Covariance Estimation to Anomaly Detection on event topology\n",
    "- Use an Isolation Forest to do Anomaly Detection on event topology\n",
    "- Visualize our basic anomaly detection methods on real CMS Data\n",
    "\n",
    "### Let's start out with some basic Anomaly Detection Examples.\n",
    "\n",
    "Anomaly Detection belongs to the subfamily of machine learning disciplines called \"Unsupervised Learning\" (yes, there is \"weakly supervised\" or \"semi-supervised\", but in the broad three group taxonomy of \"Supervised\", \"Unsupervised\" or \"Reinforcement\" learning, it leans towards unsupervised). Unlike \"Supervised Learning\" where there is a target dataset to be replicated via the machine learning acting as a function on the data you have to the dataset you would like to predict (or reinforcement learning, which is designed to solved control and optimization problems, and targets must be learned), _\"Unsupervised learning\" in general, is designed to try and fit a functional form to underlying patterns of the dataset you have with no other targets_. It may help to think about it like supervised learning, but with the target being the same dataset you feed in (this is of course, not strictly true, but close enough).  \n",
    "\n",
    "If you understand or have a good functional fit to the underlying structure of the data you are looking at, you can tell what things _do not_ fit that structure. This is anomaly detection. You may also see related or synonymous concepts \"outlier detection\" and \"novelty detection\" (as [scikit-learn puts it](https://scikit-learn.org/stable/modules/outlier_detection.html#overview-of-outlier-detection-methods), \"outlier detection\" is done on datasets that have outliers or anomalies mixed in, \"novelty detection\" learns from a dataset free from these things, and then detects anything new later)\n",
    "\n",
    "Because neural networks have exploded in popularity over the last 10 years, nearly everyone thinks about them first when referring to machine learning (and to be fair, they are nearly an ideal use case for physics, with high expressive power, but low sample efficiency being compensated by tremendous amounts of available data to use), but neural networks are not the only machine learning tool, and certainly not the simplest. Similarly, Auto-encoders of all stripes and Generative Adversarial Networks are not the only unsupervised learning technique, or only anomaly detection technqiue. \n",
    "\n",
    "We're going to try a few basic tools out, just to get a sense of what anomaly detection is like without a neural network, and we'll move on to the neural network examples shortly.\n",
    "\n",
    "First, some necessities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f396f-6d1a-41f0-aa01-7ccb4c0f95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b81039-4071-4b96-b4ba-1e3303563ce4",
   "metadata": {},
   "source": [
    "I have provided with this repository some h5 files with some preprocessed CMS open data in them. If you are not familiar with .h5 files, they are a simple data format that is a bit quicker than a ROOT file, but a bit less flexible. They are generally quite popular and useful for machine learning dataset purposes.\n",
    "\n",
    "In this repo, there is a directory `/data/basic_files/` that contains .h5 files with a few different datasets in them. These are:\n",
    "\n",
    "- `data/basic_files/ZeroBiasBasicData.h5`\n",
    "  - CMS reserves some of its datastream for a random selection of events from the beam, with no preconditions. This serves several technical purposes, allowing to have some idea of what the beam is like before any trigger bias, monitoring triggger function, and giving us a chance to preserve some interesting processes we may have missed due to our biases as physicists. As you might expect, most of the things that are in this dataset are unintersting, mostly just soft QCD processes and beam pileup, but there is no reason that that interesting processes **couldn't** be in there too. This makes it an ideal dataset to go looking for anomalies in.\n",
    "- `data/basic_files/JetHTBasicData.h5`\n",
    "  - CMS of course, has an extensive trigger system, designed to filter the huge beam rate down into something we can actually afford to keep around as data. This task is akin to anomaly deteciton (albeit, not as unbiased as we might hope). This particualr dataset comes from the subset of triggers that trigger on jets or $H_{T}$. It will be interesting to see how our anomaly detection rates the data CMS already takes.\n",
    "- `data/basic_files/TTBarBasicData.h5`\n",
    "  - This is montecarlo simluation of the $t\\bar{t}$ process. The discovery top quark has recently turned 30 years old, as of the writing of this, but these days, it is already a \"common\" signal inside LHC collider data, and we are in an era of extreme top quark precision measurements. This sample will serve as a good benchmark about how well AD helps us find some common things we know are are signals.\n",
    "- `data/basic_files/SoftQCDBasicData.h5`\n",
    "  - CMS maintains specific simulation that is designed to simulated beam background and pile-up... but unfortunately I couldn't find any in the format I need. This will serve as a next best thing. We can use this to try and gauge the extent to which our AD is just picking up the differences between MC and actual data\n",
    "- `data/basic_files/RadionBasicData.h5`\n",
    "  - This is an exotic signal I picked at random. It is \"GluGluToRadionToHHTo2B2ZTo2L2J\". Needless to say, quite exotic, but the kind of thing that AD would be searching for.\n",
    " \n",
    "The actual data recorded in these files is quite basic. In order, per event I have, the number of reconstructed jets, the number of reconstructed muons, the number of reconstructed electrons, then photons, taus, wide jets, boosted taus, then the event recorded MET. These are just some basic variables for looking at event topology, we could add additional things (and indeed will later), but they will do for now.\n",
    "\n",
    "List by index for convenience, the features I have provided are:\n",
    "\n",
    "0. nJets\n",
    "1. nMuons\n",
    "2. nElectrons\n",
    "3. nPhotons\n",
    "4. nTaus\n",
    "5. nWideJets\n",
    "6. nBoostedTaus\n",
    "7. Event MET\n",
    "\n",
    "Let's start by getting this data prepared, and taking a look at it.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9c63b-b3e2-439f-9bdb-4d7ab63ebb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> np.array:\n",
    "    with h5py.File(file_path) as the_file:\n",
    "        data = np.array(the_file['basic_event_data'])\n",
    "    return data\n",
    "\n",
    "zerobias_data = load_data('data/basic_files/ZeroBiasBasicData.h5')\n",
    "\n",
    "console.print(zerobias_data.shape)\n",
    "\n",
    "jetht_data = load_data('data/basic_files/JetHTBasicData.h5')\n",
    "\n",
    "console.print(jetht_data.shape)\n",
    "\n",
    "ttbar_data = load_data('data/basic_files/TTBarBasicData.h5')\n",
    "\n",
    "console.print(ttbar_data.shape)\n",
    "\n",
    "softqcd_data = load_data('data/basic_files/SoftQCDBasicData.h5')\n",
    "\n",
    "console.print(softqcd_data.shape)\n",
    "\n",
    "radion_data = load_data('data/basic_files/RadionBasicData.h5')\n",
    "\n",
    "console.print(radion_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ddd24-b1cb-4ef0-ad82-4418c9b6379d",
   "metadata": {},
   "source": [
    "(I find it good practice when working with numpy data, and preparing for machine learning tasks to print out the shape of the data I am working with, just so there are no mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4c445-01fb-4b1d-a804-a66c28abc681",
   "metadata": {},
   "source": [
    "Alright. This is where you start coming in. Let's take a look at what this data looks like (also a good practice before doing machine learning stuff). Lets make a few 1D histograms and scatter plots of how objects are distributed in zero bias data. I have included `matplotlib` for this purpose. If you, like me, are a relative newcomer to matplotlib over ROOT, I'll leave these links here:\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "\n",
    "and give you a couple basic examples.\n",
    "\n",
    "(yeah, you could get ChatGPT to do this, or even google's search AI, and I'm not going to tell you not to use these tools, but I want to stress the difference between getting some LLM to teach you the basics, teach what you would need to go look up, giving you a skeleton to expand upon and helping you to do it for yourself, and just getting the LLM to do things for you. You do learn differently in those two cases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be77ad2-dfdb-4bfa-81f2-34e5d34efdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    zerobias_data[:, 0],\n",
    "    bins=7,\n",
    "    range=(0,7),\n",
    "    density=True,\n",
    "    histtype='stepfilled'\n",
    ")\n",
    "plt.xlabel(\"# of Jets\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.title(\"nJets ZeroBias\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab05f59-621f-44ee-89b2-74c9185fb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    zerobias_data[:, 0],\n",
    "    zerobias_data[:, 1],\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"# of Jets\")\n",
    "plt.ylabel(\"# of Electrons\")\n",
    "plt.title(\"Jets vs Electrons in Zero Bias\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38824e5d-a3d2-4094-8dff-33216082eef5",
   "metadata": {},
   "source": [
    "Play around a bit, get a feel for how the variables are related, or aren't (you could probably write a couple of helper functions to cut down on the amount of code you have to write):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a37f46-63c3-4994-be54-896a1037845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make some more plots of Zero Bias Data\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a83f1-d7cd-45a6-aa27-ca88ac6b19e7",
   "metadata": {},
   "source": [
    "Okay. Now let's do some machine learning.\n",
    "\n",
    "### Basic AD:  Covariance Estimation\n",
    "\n",
    "One of the simplest methods for doing anomaly detection relies on estimating the variance/covariance of data. This is a reasonable enough approximation, as the Gaussian Normal Distribution is of course an _incredibly_ common way to find data distributed for complex or inherently probabilistic systems. The idea is simple enough and should sound a little familiar, assume/fit a normal distribution to the data, and things far enough outside the boundary defined by the standard deviation (technically in this case, the Mahalannobis Distance) are outliers. But there is a slight twist. Outliers/anomalies tend to have an outsized effect on the covariance estimation. To get around it, a reduced set of data, with conditions on the covariance matrix, is used (Minimum Covariance Determinant) to get a more robust estimation of what the covariance for the most central set of data looks like.\n",
    "\n",
    "You're going to try it out. Make an sklearn pipeline, and fit it to our zero bias data. [Here's the scikit-learn page for the EllipticEnvelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope)\n",
    "\n",
    "To help out with the fit (it can be a bit unstable due to the high number of points), I [split the zero bias data into a train set](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) of about 1/8th of the data and a test set of about 7/8ths of it\n",
    "\n",
    "(A few hints: \n",
    "\n",
    "It is always good practice to normalize all your data before doing model fitting!\n",
    "\n",
    "The fit is a little picky due to the high number of points it is using to do matrix math/covariance estimation. If it gives you tons of warnings the fit likely broke. Set the `support_fraction` a bit higher in the `EllipticEnvelope` declaration. This uses more points in the covariance estimation, and makes it a touch easier for anomalies to be included in the covariance estimation, but it helps the fit out. Start with `support_fraction=0.6`\n",
    "\n",
    "The elliptic envelope has a hyperparameter! `contamination`, the amount of expected anomalies. Try playing around with it!\n",
    "\n",
    "If you have extra time, try playing around with other preprocessing ideas, like PCA. I haven't tested it, but it might be interesting\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ead8e-6134-4bed-8a95-1c2f1062a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zerobias_train, zerobias_test = train_test_split(\n",
    "    zerobias_data,\n",
    "    train_size= (100000)/len(zerobias_data),\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfdd60-9380-4771-878a-211aa07c7fb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a covariance envelope estimator\n",
    "#\n",
    "\n",
    "covariance_model = Pipeline( # takes list of tuples (name of step, estimator) as input\n",
    "    [\n",
    "        (\n",
    "            \"normalization\",\n",
    "            # fill in here\n",
    "        ),\n",
    "        (\n",
    "            \"covariance_AD\",\n",
    "            # fill in with EllipticEnvelope here\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# I am not making an effort to split this into a train_test set, although, you could do that, and indeed it might be good to do so.\n",
    "start_fit = perf_counter()\n",
    "covariance_model.fit(zerobias_train)\n",
    "end_fit = perf_counter()\n",
    "console.print(f'Completed fit in: {end_fit-start_fit:0.2g} Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e075a-df93-4af4-a44b-bc8d3ea30e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f00261-3be4-4be5-9a95-f35aba4be56e",
   "metadata": {},
   "source": [
    "Great, Now we have a model for anomalous data in the CMS beam! What does that look like? Let's predict which of our events are anomalous, and figure out where they are!\n",
    "\n",
    "(\n",
    "A few hints and things to check out:\n",
    "\n",
    "We set the amount of anomlies we expected to see as a hyper-parameter. Did we actually get that much when we predict on zero bias data?\n",
    "\n",
    "Look at the anomalies as a histogram in nJets first (two histograms, with alpha = 0.5). Any features that stick out to you?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff09c4f-ee8e-4359-85c5-3bea2d8cd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make predictions on our zero bias dataset and plot the events that are anomalous\n",
    "#\n",
    "zerobias_covariance_anomaly_mask = # make predictions on zerobias_test using covariance_model\n",
    "zerobias_covariance_nonanomalies = zerobias_test[zerobias_covariance_anomaly_mask == 1]\n",
    "zerobias_covariance_anomalies = zerobias_test[# fill in here]\n",
    "\n",
    "#console.print(len(zerobias_covariance_anomalies)/len(zerobias_data))\n",
    "#console.print(len(zerobias_data))\n",
    "\n",
    "plt.hist(\n",
    "    # fill in (histogram of njets for non-anomalous events)\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # fill in (histogram of njets for anomalous events)\n",
    ")\n",
    "\n",
    "plt.xlabel('nJets')\n",
    "plt.ylabel('Events')\n",
    "plt.title('Anomalous event in Zero Bias by nJets')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fe27a-a7d2-46ac-83cd-594c6b163304",
   "metadata": {},
   "source": [
    "Here's an advanced challenge, feel free to skip it, but see if you draw out the 2D decision boundaries as [shown here](https://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py) for the numbers jets and MET, and number of jets and number of electrons.\n",
    "\n",
    "It doesn't seem to work very well because we've fit our model on more than 2 variables. Many things that are non-anomalous in jets or MET, end up anomalous for other reasons. You can try to offset integer variables for the non-anomalous plot just slightly to get a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441097ff-992e-41d1-bb5a-5a1618307ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Optional Exercise: Plot decision function curves for a few combinations of variables\n",
    "#\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "\n",
    "emp_cov = EmpiricalCovariance()\n",
    "min_cov = MinCovDet()\n",
    "\n",
    "emp_cov.fit(np.stack([zerobias_test[:, 0], zerobias_test[:, 7]], axis=1))\n",
    "min_cov.fit(np.stack([zerobias_test[:, 0], zerobias_test[:, 7]], axis=1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "non_anomalous_plot = plt.scatter(\n",
    "    # fill in with nonanomalous covariance\n",
    "    # hint: offset the dots by a small amount in one dimension so that all data is visible\n",
    ")\n",
    "\n",
    "anomalous_plot = plt.scatter(\n",
    "     # fill in with anomalous covariance\n",
    ")\n",
    "\n",
    "#Make the mesh grid\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n",
    "    np.linspace(plt.ylim()[0], plt.ylim()[1], 100),\n",
    ")\n",
    "\n",
    "zz = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Calculate the MLE based Mahalanobis distances of the meshgrid\n",
    "mahal_emp_cov = # fill in here using mahalanobis method on emp_cov\n",
    "mahal_emp_cov = # reshape to xx's shape\n",
    "emp_cov_contour = plt.contour(\n",
    "    xx, yy, np.sqrt(mahal_emp_cov), cmap=plt.cm.PuBu_r, linestyles=\"dashed\"\n",
    ")\n",
    "# Calculate the MCD based Mahalanobis distances\n",
    "mahal_robust_cov = # fill in here using mahalanobis method on min_cov\n",
    "mahal_robust_cov = # reshape to xx's shape\n",
    "robust_contour = ax.contour(\n",
    "    xx, yy, np.sqrt(mahal_robust_cov), cmap=plt.cm.YlOrBr_r, linestyles=\"dotted\"\n",
    ")\n",
    "\n",
    "ax.legend(\n",
    "    [\n",
    "        mlines.Line2D([], [], color=\"tab:blue\", linestyle=\"dashed\"),\n",
    "        mlines.Line2D([], [], color=\"tab:orange\", linestyle=\"dotted\"),\n",
    "        non_anomalous_plot,\n",
    "        #anomalous_plot,\n",
    "    ],\n",
    "    [\n",
    "        \"True Covariance Estimate\", \n",
    "        \"Robust Covariance Estimate\", \n",
    "        \"non-anomalous\", \n",
    "        \"anomalous\"\n",
    "    ],\n",
    "    loc=\"upper right\",\n",
    "    borderaxespad=0,\n",
    ")\n",
    "\n",
    "ax.set_xlabel('nJets')\n",
    "ax.set_ylabel('MET')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db93afb-4eb6-4b74-8df7-d9fd8ce152f3",
   "metadata": {},
   "source": [
    "As I mentioned above, a great deal of effort already goes into doing a kind of anomaly detection on the LHC beam as it goes into CMS. That's the whole job of the trigger system. We have to narrow down a ~28 MHz rate to a few kHz we can actually save. How does our anomaly detection compare to anomaly detection already done? Well, let's make predictions on that data. Do we find it anomalous? First, try just figuring out how many events in the data set are rated anomalous (as a percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2dc7af-bf47-4922-b0f8-45f56e5fb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Figure out what percent of the Jet HT dataset is anomalous in the covariance model\n",
    "#\n",
    "\n",
    "jetht_covariance_anomaly_mask = # create prediction for jetht_data using covariance_model\n",
    "\n",
    "jetht_anomalies = # filter for anomalous events using above mask\n",
    "jetht_nonanomalies = # filter for nonanomalous events using above mask\n",
    "\n",
    "console.print(f'The Jet HT dataset is: {len(jetht_anomalies)/len(jetht_data):0.2%} anomalous according to our zero bias model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fffd62-6145-4eaf-92bd-35c1b62728d4",
   "metadata": {},
   "source": [
    "This is interesting, but remember how we set an amount of anomalies we wanted to see before? We could keep retesting this for any number of hyper-parameters...\n",
    "\n",
    "Remember, that anomaly detection is attempting to make an unsupervised classifier (in effect) though! We can talk about like a classifier (between anomalous and not), and use metrics that are common to supervised classifiers! In this case, the ROC curve (Receiver Oeprating Characteristic if you are not familiar, a curve/metric that basically asks, for a given score, how much of my signal do I get versus how much of my background do I let in, the goal being maximum signal for minimum background, typically denoted as AUC, Area-Under-the-Curve, with higher being better). ROCs really need a score per sample to be used though, so what do we use? Well, the elliptic curve provides a method `score_samples()` that it uses to generate scores (based on the Mahalanobis distances) it cuts on for the hyperparameter we set early. We can just get direct scores, and use that as a score per sample (it actually returns the negative, so we just add another negative sign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8403c0-1467-4b65-bbc6-1806f5d7e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da7db0-ff9c-4486-9692-4fd231c071ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a ROC curve of Jet HT versus Zero Bias\n",
    "#\n",
    "\n",
    "jetht_score = -1.0*covariance_model.score_samples(jetht_data)\n",
    "zerobias_score = # fill in here\n",
    "\n",
    "y_true = # np array with ones corresponding to jet ht entries and zeros corresponding to zero bias entries\n",
    "y_pred= # np array containing scores from jet ht and zero bias, lined up with the ones and zeros in the above array\n",
    "\n",
    "# compute fpr and tpr using the roc_curve method with y_true and y_pred\n",
    "# compute the auc using the roc_auc_score method with y_true and y_pred\n",
    "\n",
    "plt.plot(\n",
    "    # plot fpr vs tpr (can print the auc somewhere on the plot for reference)\n",
    ")\n",
    "plt.xlabel('False Positive Rate (Zero Bias Acceptance)')\n",
    "plt.ylabel('True Positive Rate (Jet HT Dataset Accepted)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5ed2a-080a-4461-ba12-78faf7f008be",
   "metadata": {},
   "source": [
    "Not bad huh? Just some math about distribution variance and we're already selecting the better part of an interesting physics dataset making **no** assumptions other than we think most beam events are pretty uninteresting.\n",
    "\n",
    "There's a lot of cool things about this method already. For one, it is so simple, with a bit of work, it could be implemented just about anywhere, even in HLS on an FPGA if you were so inclined. There's a lot of other things we could discuss about this, and I plan to, but let's introduce another Anomaly detection type first and then talk about the various points we should note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e036d35-39ca-42a5-82e0-b9e091967789",
   "metadata": {},
   "source": [
    "### Basic AD: Isolation Forest\n",
    "\n",
    "Another simple model you can use to look for anomalies is an \"Isolation Forest\". The idea behind this goes like so: imagine a scatterplot of a 2D dataset with some outliers. The non-outliers in that dataset are, by definition, going to closer to themselves, and relatively tightly packed in compared to the outliers. Now imagine splitting the dataset in two with a random cut along one variable/axis. If you keep cutting the dataset, it takes far fewer cuts to isolate an anomalous data point away, than one of the non-anomalous ones, because of the different densities. The number of cuts (or path length along the tree of cuts) is different for anomalies and non-anomalies, giving us a way to rank the anomalous-ness of a data-point and separate them. This can be done over many random trees to produce a generalized anomaly model of our data. This type of model with many trees is our \"Isolation Forset\" (this is highly related to the decision tree series of models that are still popular, and it's ensemble equivalents, the random forest).\n",
    "\n",
    "[Here's a link to the scikit-learn API](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest). You're going to make one.\n",
    "\n",
    "(A few hints, things to look at:\n",
    "\n",
    "Scale your data!\n",
    "\n",
    "the isolation forest has a `contamination` hyperparameter also. It also has an 'auto' method, based on it's original paper. I recommend setting it to 0.05 by hand.\n",
    "\n",
    "like random forests/most ensemble models, isolation forests have a hyperparameter including the number of base estimators `n_estimators`. Try playing around with that\n",
    "\n",
    "I find forest/tree techniques depend _heavily_ on preprocessing and especially choice of input features. Try playing around with that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c914db-bbe5-4e8e-be92-1683d6a2cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#We can use a slightly bigger set of training data here\n",
    "zerobias_train, zerobias_test = train_test_split(\n",
    "    zerobias_data,\n",
    "    test_size= 0.4,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757cad7-4f3f-461b-aab6-6b021b1cbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make an Isolation forest Anomaly Detection Model\n",
    "#\n",
    "\n",
    "isoforest_model = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            'scaler',\n",
    "            # fill in here\n",
    "        ),\n",
    "        (\n",
    "            'isolation_forest',\n",
    "            # fill in here with IsolationForest\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "start_fit = perf_counter()\n",
    "isoforest_model.fit(zerobias_train)\n",
    "end_fit = perf_counter()\n",
    "console.print(f'Completed fit in: {end_fit-start_fit:0.2g} Seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b28cbc-d24f-43e4-9d34-a4bcc188abf2",
   "metadata": {},
   "source": [
    "As before, let's take a look at what events are anomalous. For variety's sake, let's look at nElectrons and MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768b8c6-bb15-4b10-beb5-a27d40c2562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zerobias_isoforest_anomaly_mask = # compute predictions on zero bias test data using isoforest_model\n",
    "zerobias_isoforest_nonanomalies = # filter test data by nonanomalous events\n",
    "zerobias_isoforest_anomalies = # filter test data by anomalous events\n",
    "\n",
    "plt.hist(\n",
    "    # histogram of nElectrons for nonanomalous events\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # histogram of nElectrons for anomalous events\n",
    ")\n",
    "\n",
    "plt.xlabel('nElectrons')\n",
    "plt.ylabel('Events')\n",
    "plt.yscale('log')\n",
    "plt.title('Anomalous event in Zero Bias by nElectrons')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.hist(\n",
    "    # histogram of MET for nonanomalous events\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    # histogram of MET for anomalous events\n",
    ")\n",
    "\n",
    "plt.xlabel('MET')\n",
    "plt.ylabel('Events')\n",
    "plt.yscale('log')\n",
    "plt.title('Anomalous event in Zero Bias by MET')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fc08c-752e-4ae1-8b67-1b8faf305bd0",
   "metadata": {},
   "source": [
    "A thing we haven't done yet is take a look at how well our Anomaly detection can find signals, things that could be new physics. Now, we should be a bit careful here that we don't introduce bias into our evaluation of models by tuning them for specific signals (or training a supervised model to find one!) but we can ask how well our models do. Our stand-in here will be the $t\\bar{t}$ process. As before, let's take a look at ROC curves on the Jet HT set, and the $t\\bar{t}$ data. You can use `score_samples` again (in this case, it is doing math on the path length to the leaf containing an observation, but has adjusted it so high score is high anomalousness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803ecb8-4664-4207-880e-016267f6a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a ROC curve on the Jet HT set and ttbar set with respect to zero bias\n",
    "#\n",
    "\n",
    "ttbar_score = # fill in using score_samples method, applied like we did before\n",
    "jetht_score = # fill in using score_samples method, applied like we did before\n",
    "zerobias_score = # fill in using score_samples method, applied like we did before\n",
    "\n",
    "print(ttbar_score)\n",
    "print(jetht_score)\n",
    "print(zerobias_score)\n",
    "\n",
    "y_true = # array with ones corresponding to jet ht and zeros corresponding to zero bias\n",
    "y_pred = # array with jet ht and zero bias scores aligned with the above array\n",
    "\n",
    "# compute fpr, tpr, and aux\n",
    "\n",
    "plt.plot(\n",
    "    # plot fpr vs tpr\n",
    ")\n",
    "\n",
    "# add an additional plot to this axis where ttbar is the signal instead of jet ht (can format code in the same way that you did above)\n",
    "y_true = # fill in\n",
    "y_pred = # fill in\n",
    "\n",
    "plt.plot(\n",
    "    # fill in\n",
    ")\n",
    "\n",
    "plt.xlabel('False Positive Rate (Zero Bias Acceptance)')\n",
    "plt.ylabel('True Positive Rate (Signal Accepted)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71bd0b3-7e4d-4472-8b44-d596f4b72504",
   "metadata": {},
   "source": [
    "You should find that we're doing pretty good again! In fact, just based on the AUC for Jet HT, we are probably doing _better_ than the covariance based methods (likely due to a slightly more accurate to the underlying distribution fit than just assuming everything is normally distributed).\n",
    "\n",
    "And on the $t\\bar{t}$ signal you should find we do incredibly well!\n",
    "\n",
    "Another thing we could ask is, \"How well does this method do over existing triggering methods?\". We can make our roc curve against the Jet HT dataset instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df2c32-17f6-4630-b02d-222a006151d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: make a ttbar ROC curve with respect to the Jet HT set\n",
    "#\n",
    "\n",
    "# you should have enough hints from the above exercises :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd7f35-cf60-4ec1-84c7-9c048c7f0c9d",
   "metadata": {},
   "source": [
    "This isn't even half bad at finding $t\\bar{t}$ in a very specifically jet selected dataset!\n",
    "\n",
    "One of the upsides of this minimally biased, unsupervised technique is that it is maximally applicable. We can potentially use it to find _any_ signal. To demonstrate, let's make a ROC curve wtih the Radion sample and $t\\bar{t}$ sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62857d44-81d7-4941-a8f8-75fe32ec0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make two ROC curves, ttbar and radion, with zero bias as background\n",
    "#\n",
    "\n",
    "radion_score = # compute score from radion_data like we did with the other signal samples above\n",
    "\n",
    "# compute y_true, y_pred, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f30ac7-9af4-4db6-803a-3dafce125e22",
   "metadata": {},
   "source": [
    "But here's the complication. We've been evaluating our MC performance against data. It's possible our model is just picking up on the difference between data and MC, not the difference between uninteresting events and exciting physics. This is why I have included the Soft QCD dataset. It isn't a perfect stand in for the beam background simulation, but it should give us an idea how much of our signal finding power is just a function of Data and MC looking kind of different anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755ed57-8f53-4807-ab4c-eb5a6ebcc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a ROC curve of the ttbar dataset and Radion set with respect to the soft QCD set\n",
    "#\n",
    "\n",
    "qcd_score = # comnpute score from softqcd_data\n",
    "\n",
    "\n",
    "y_true = # ones correspond to radion_score, zeros correspond to qcd_score\n",
    "y_pred = # contains radion score and qcd score and aligns with above array\n",
    "\n",
    "# compute fpr, tpr and auc\n",
    "\n",
    "plt.plot(\n",
    "    # plot fpr vs tpr and include auc in the label\n",
    ")\n",
    "\n",
    "# add another plot where ttbar is the signal and soft QCD is the background\n",
    "\n",
    "plt.xlabel('False Positive Rate (QCD Acceptance)')\n",
    "plt.ylabel('True Positive Rate (Signal Accepted)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be16abf-00bb-4086-b04e-b903f8a9edf9",
   "metadata": {},
   "source": [
    "You should find that we're still doing okay, but the performance is just a touch worse than when we evaluated against data. If we had proper simulation of the beam background available, I would expect it to still be a pretty good method, but maybe not as good as we see here, picking up a bit on the difference between simulation and true data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706e8db-f9dd-4958-bfd9-7a03792aa184",
   "metadata": {},
   "source": [
    "### Wrap-up\n",
    "\n",
    "There's a lot of cool stuff in AD and basic data-science style anomaly detection that I just won't have time to get all the way into. I highly recommend taking a read through of SciKit-Learn's [Outlier and Novelty Detection](https://scikit-learn.org/stable/modules/outlier_detection.html#id1) section to get a sense of what sort of things are out there. If you can get away with not using a neural network for simplicity reasons, or a neural network just doesn't out-perform the simple stuff, you should be aware that these techniques are out there.\n",
    "\n",
    "That being said, I love some neural network techniques, and I'm willing to bet you want to see some too. Let's talk about them in the next exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
