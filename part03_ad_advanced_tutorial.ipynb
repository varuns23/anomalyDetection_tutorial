{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f94760-55c5-472f-b26f-36b636eacc4e",
   "metadata": {},
   "source": [
    "# Exercise 4: Advanced AD with Graph Neural Networks And Graph Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9711f-8b77-4d4f-9405-46f984c31a77",
   "metadata": {},
   "source": [
    "### Goals of the Exercise\n",
    "- Make a Graph Autoencoder for event data, and use it for anomaly detection in events\n",
    "\n",
    "### Graphs, and Graph Autoencoders\n",
    "\n",
    "If you did the last exercise, and took a look at the grid based event data I provided, no doubt a great deal of it was just 0's. It's a lot of wasted space, math, and CPU/GPU time looking at those cells. And it doesn't really capture the _true_ structure of an event to have that whole grid there. Some events may have dozens of particles. Some 1 or 2. But we use the same grid each time. And the inconsistent and sparse nature of the input probably isnt doing our autoencoder any favors either.\n",
    "\n",
    "To get around the shortcomings of this kind of input, and to get a real model of varying amounts of data, graphs are often used. The strict math definition of a graph is just some structure with both vertices and edges, but for our practical purposes, we will think of those vertices as also having features (and sometimes even edges will have features!), and when we say vertices mostly what we mean is particles (but it of course extends to other kinds of data that can be modeled by points with connections). Graphs have their own kind of neural networks called [Graph Neural Networks](https://en.wikipedia.org/wiki/Graph_neural_network). Graph neural networks are typically \"message passing\", that is a layer will accumulate information from it's neighbor vertices (up to a certain distance away), do standard neural network layer operations on this plus the node in question's information, and come up with a new feature for the next layer of the neural network/graph model. Graph neural networks can be used to make predictions per vertex, or even be entirely and become a standard flat neural network providing predictions about the graph as a whole.\n",
    "\n",
    "Graph neural networks also have graph autoencoders. In fact, due to the complicated nature of graphs, there are conceptually a number of different kinds you can create and combine with traditional auto-encoder concepts. The most common kind is an edge-based auto encoder, that processes the node information into a flat vector per node, then simply tries to reconstruct the graph edges by inner products of the latent-space nodes, with binary-cross entropy being the loss for \"edge\" or \"not edge\" classifications. One could conceivably also, with enough information per node, ask the neural network to reconstruct the original node features, and ignore the graph edges.\n",
    "\n",
    "For our exercise we are changing it up a bit and going to be using [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), an extension of the popular [Torch](https://pytorch.org/) neural network library. There does exist a graph neural network library for Tensorflow/Keras (Keras v2) called [Spektral](https://graphneural.network/) but in all honesty I have never used it, and it doesn't seem as complete, or as updated as PyTorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758bc3c-5ae5-4277-8907-905be0a02af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "console=Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6ff86-cb83-473a-962b-04b64f045f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "zerobias_tensors = torch.load('data/graph_files/ZeroBiasGraphData.pt', weights_only=False)\n",
    "jetht_tensors = torch.load('data/graph_files/JetHTGraphData.pt', weights_only=False)\n",
    "\n",
    "np.random.shuffle(zerobias_tensors)\n",
    "\n",
    "zerobias_train_tensors = zerobias_tensors[:round(0.6*len(zerobias_tensors))]\n",
    "zerobias_val_tensors = zerobias_tensors[round(0.6*len(zerobias_tensors)):round(0.8*len(zerobias_tensors))]\n",
    "zerobias_test_tensors = zerobias_tensors[round(0.8*len(zerobias_tensors)):]\n",
    "\n",
    "#zerobias_load = DataLoader(zerobias_tensors, batch_size=32, shuffle=True)\n",
    "\n",
    "zerobias_train_loader = DataLoader(zerobias_train_tensors, batch_size=32, shuffle=True)\n",
    "zerobias_val_loader = DataLoader(zerobias_val_tensors, batch_size=32, shuffle=True)\n",
    "zerobias_test_loader = DataLoader(zerobias_test_tensors, batch_size=32, shuffle=True)\n",
    "jetht_load = DataLoader(jetht_tensors, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd93ce4-cf8d-4f03-bcbf-838dbacd3759",
   "metadata": {},
   "source": [
    "In these files, I have provided graph tensors (a combination of nodes of features listed out as vectors, and an adjacency matrix listing the connections) of the 40 highest $p_{T}$ Particle Flow (PF) Candidates from a set of CMS events. Particle Flow Candidates are the base level of reconstruction at CMS, even before the more common reco objects like jets (PF hadrons are later clustered into jets or taus). The graph has been constructed as a k-nearest neighbors graph (with k = 3) in $\\phi$ and $\\eta$. There are 5 features provided by node, in order they are:\n",
    "\n",
    "0. $p_{T}$\n",
    "1. $\\eta$\n",
    "2. $\\phi$\n",
    "3. $m$\n",
    "4. PDG ID\n",
    "\n",
    "There are typically _far_ more PF Candidates per event than 40 (I counted Zero bias at on average having $\\approx 900$, and Jet/HT at $\\approx 1600$), but because these tensors can get _very_ large and the files containing them as well, I have had to restrict it to those with the most energy in the event, and relatively few features. Partially for this same reason, and partially because they are the only files with PF Candidates, I have also only provided ZeroBias and Jet HT datasets.\n",
    "\n",
    "### Making a Graph Autoencoder\n",
    "\n",
    "Making this autoencoder is a bit different from a keras model. [Pytorch geometric provides an inner-product decoder model/oberarching auto-encoder model](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html#torch_geometric.nn.models.GAE), we simply need to provide the encoder ourselves. [The encoder is written up as a `torch.nn.Module`, which provides the `forward` function that defines what a forward pass of the neural network looks like](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html). The simplest message passing layers to use are [`GCNConv` layers](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv), but graph neural networks have [_many_ possible kinds of layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers) compared to relatively fewer for standard neural networks, and they invent new ones all the time for various applications. We will start with `GCNConv` for now, but if you have the time, play around with others\n",
    "\n",
    "Some things you can play around with here:\n",
    "\n",
    "- The latent space for a graph auto-encoder is a bit funny. Because you are doing inner-products and determining the probability an edge exists, the latent space merely needs to be smaller than the smallest possible way of denoting every possible connection. which is potentially quite large. But this means it possibly helps for the features per node to _expand_ instead of contract, like you may be used to with a regular auto-encoder\n",
    "- If you're really feeling quite adventurous, and familiar with pytorch and pytorch geometric, something I've always wanted to try is a _node-wise_ auto-encoder. Instead of attempting to reconstruct edge connections, instead make a more typical graph neural network that compresses each node into a latent space, then attempts to decompress it. I have literally never tried this (but always wanted to to test its capabilities), so it might be interesting to check how this functions. You have relatively limited features per node to work with, but, it may be worth a shot to see how it functions with a basic MSE loss per node compared to binary cross entropy per edge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e715c4-192f-4fb6-a051-4fd55a79cdb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Hints (expand to see more)\n",
    "- use `remove_self_loops` and `negative_sampling` to get positive and negative edge indices for loss calculations\n",
    "- the GAE model provides it's own `forward` for the forward pass of a neural network, and even a `recon_loss` to calculate it's own loss. Use those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061a6e8-5e9f-4dba-b27f-a2b50564ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch_geometric.nn.models import GAE\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b695e0-2762-4829-9dbc-022eab3ea16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Make a Graph Autoencoder model. Don't worry about training it yet\n",
    "#\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # fill with Graph Neural Network Layers\n",
    "    def forward(self, x, edge_index):\n",
    "        x=x.to(torch.float)\n",
    "        edge_index = edge_index.to(torch.int)\n",
    "\n",
    "        #run the inputs/edge indicies through the layers\n",
    "        return #return the final tensors/edge indices\n",
    "\n",
    "encoder = GCNEncoder(in_features=5, out_features=\"Fill me!\")\n",
    "model = GAE(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e01549-e608-4215-bddf-83844748878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: write the training loop\n",
    "#\n",
    "\n",
    "from rich.progress import track\n",
    "import torch.optim as optim\n",
    "from torch_geometric.utils import remove_self_loops, negative_sampling\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Because pytorch is a little more hands off, our training loop needs to be handcrafted\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in track(zerobias_train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #Actual do the encode and decode step in the neural network\n",
    "        z = #fill me with model evaluation\n",
    "\n",
    "        #Get the edge indices so that we can compute the loss on the decoded graph\n",
    "        pos_edge_index, _ = #fill me\n",
    "        neg_edge_index = #fill me\n",
    "\n",
    "        #do some casts to prevent any type issues\n",
    "        pos_edge_index = pos_edge_index.to(torch.int)\n",
    "        neg_edge_index = neg_edge_index.to(torch.int)\n",
    "        \n",
    "        #Get the loss from the model's recon_loss() function\n",
    "        loss = #fill me with the model loss!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(zerobias_train_loader)\n",
    "\n",
    "# we use a similar loop for validation measurements\n",
    "def train_loop_validation():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    #Fill me! No optimizer steps needed\n",
    "    \n",
    "for epoch in range(1,11):\n",
    "    loss = train()\n",
    "    console.print(f'Epoch {epoch}, loss: {loss}')\n",
    "    val_loss = train_loop_validation()\n",
    "    console.print(f'Epoch {epoch}, validation loss: {val_loss}')\n",
    "    console.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f07f81-6021-4a4d-be4c-cade7fa86f73",
   "metadata": {},
   "source": [
    "Okay. We have a model. Now we need to use it and figure out some anomaly scores. I will provide a function for running on the jetht_dataloader, you provide the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09daaa-c971-4cd5-8b18-7286d9e5b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_per_graph(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    \n",
    "    for data in data_loader:\n",
    "        data.to(device)\n",
    "        z=model.forward(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "        #Split the batch into distinct graphs\n",
    "        batch_size=data.num_graphs\n",
    "        edge_index=data.edge_index\n",
    "        node_splits = torch.unique(data.batch, return_counts=True)[1].cumsum(0)\n",
    "        node_splits = torch.cat([torch.tensor([0], device=device), node_splits])\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Mask to get nodes/edges for graph i\n",
    "            node_start, node_end = node_splits[i], node_splits[i+1]\n",
    "            node_mask = (data.batch == i)\n",
    "            node_indices = torch.arange(node_start, node_end, device=device)\n",
    "\n",
    "            # Filter edges belonging to this graph\n",
    "            src, dst = edge_index\n",
    "            edge_mask = node_mask[src] & node_mask[dst]\n",
    "            pos_edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "            if pos_edge_index.size(1) == 0:\n",
    "                losses.append(torch.tensor(float('nan'), device=device))\n",
    "                continue\n",
    "\n",
    "            # Negative sampling for this graph\n",
    "            neg_edge_index = negative_sampling(\n",
    "                edge_index=pos_edge_index,\n",
    "                num_nodes=node_mask.sum().item(),\n",
    "                num_neg_samples=pos_edge_index.size(1)\n",
    "            )\n",
    "\n",
    "            # Reconstruct with masked z\n",
    "            z_i = z[node_mask]\n",
    "\n",
    "            # Compute loss for this graph\n",
    "            loss = model.recon_loss(z_i, pos_edge_index - node_start, neg_edge_index)\n",
    "\n",
    "            #loss = -torch.log(pos_pred + 1e-15).mean() - torch.log(1 - neg_pred + 1e-15).mean()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return losses  # List of float losses (1 per graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8cadb-7cd7-4b87-a3cd-b9eb928b262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Exercise: Use the above function to make some losses on our test sets, and look at how well our graph autoencoder does for anomaly detection\n",
    "#\n",
    "\n",
    "zerobias_losses = # fill me\n",
    "jetht_losses = # fill me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c3140-6bcd-4a5c-9558-22a148a1b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Make some ROCs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e72a-c079-4818-9d8c-65aca66314bc",
   "metadata": {},
   "source": [
    "Hmmm. If your thing is anything like mine the AUC will be _okay_ but not really _inspiring_ ($\\approx 0.814$) compared to some things we've seen. Don't let this sour you on the idea of graphs for anomaly detection. There were a lot of sacrifices that had to be made to get this to work at all (few PFCands, few features, few examples). In addition, the graph I made for you was quite basic, inspired by nothing other than physical distance in the detector coordinates most frequently used, and it really wasn't all that interconnected either, with each PF Candidate only connected to 3 other particles. It could even be that our graph autoencoder is _too good at physics_ and we are experiencing significant out of set reconstruction.\n",
    "\n",
    "In fact, with as few PF cands (possibly less than could be contained in a jet!) that we've used, it is kind of good that it works this well at this point.\n",
    "\n",
    "There are any number of things that could be tweaked with this approach to get it to perform a bit better. We could change the graphs we use to be more expressive. More features per node would be good too, as would more examples to train (the space requirements for these graphs can grow fast). We could try more complicated models or layers. The combinations may not be endless, but graph autoencoders come with a ton of expressive power, and consequently a huge number of things that could be tuned. Keep giving it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d6216-e0bd-4794-87e8-1215b57dd177",
   "metadata": {},
   "source": [
    "### Wrap Up\n",
    "\n",
    "Congratulations! This is just about everything I could think up for fluency in anomaly detection on physics data. It is not every topic out there, it is an active field of research after all, but if you were unfamiliar with anomaly detection before hopefully this can get you started, and if you were working with anomaly detection already, hopefully this has given you a few ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fd234-24bf-4a6f-a532-3b0e7001740b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
